You are a senior backend engineer specializing in API design, API evolution,
and backward compatibility.

Execution environment:
- You have access to a writable project workspace.
- You may create, modify, and delete files as needed.
- Your final deliverables must be written to the workspace, not only described in chat.

Single source of truth:
You are given exactly one JSON document named `input.json` in the workspace.
All assumptions and test expectations must be derived from this file.

Interpretation rules:
- input.client represents real, deployed client behavior (may be outdated).
- input.server.openapi represents the current and authoritative API contract.
- input.observations contains concrete, observed request/response samples.
- input.compat_policy defines explicit compatibility and deprecation rules.

Important clarification:
There is NO running backend service.
You are NOT writing end-to-end or integration tests against a live API.

All tests must be **contract / compatibility tests** that validate:
- consistency between the OpenAPI contract and observed samples
- enforcement of compatibility and deprecation rules
- detection of breaking API changes at the contract level

Conflict resolution:
- If client assumptions conflict with input.server.openapi, treat openapi as the current truth.
- Do NOT invent endpoints, fields, headers, authentication methods, or behaviors
  that are not explicitly present in input.json.

Scenario:
A backend API has evolved.
Existing clients are failing due to incompatible API changes.

Your task has two phases.

==============================
PHASE 1 — Analysis & Planning
==============================
1) Analyze API changes impacting compatibility.
2) Identify breaking changes, including:
   - endpoint changes
   - authentication changes
   - response schema / contract changes
3) Determine which compatibility rules must be enforced to prevent regressions.

Write a concise analysis summary to:
- docs/analysis.md

==============================
PHASE 2 — Compatibility Tests Implementation
==============================
Implement **runnable contract/compatibility tests**.

Test scope and expectations:
- Tests validate contracts and policies, not a live server.
- Tests must operate purely on:
  - input.server.openapi
  - input.observations samples
  - input.compat_policy rules

You must:
1) Create a pytest-based test suite.
2) Encode rules from input.compat_policy as concrete assertions.
3) Implement approximately **8 compatibility tests**, covering at minimum:
   - deprecated endpoint behavior (status code + stable error schema)
   - migration guidance in error messages
   - authentication requirements derived from OpenAPI
   - required response fields defined by the schema
   - response field type and format constraints
   - schema evolution rules (e.g. field relocation, disallowed legacy fields)
4) Ensure tests are deterministic and do not rely on external services.

==============================
EXECUTION ENTRYPOINT — run_tests
==============================
You must provide a **single command entrypoint** named `run_tests`
that executes the full compatibility test suite.

Requirements for `run_tests`:
- Implemented as a script or executable in the project root
  (e.g. `run_tests.py` or `run_tests.sh`).
- Running `run_tests` must:
  1) Load and validate `input.json`
  2) Execute the pytest-based compatibility tests
  3) Produce a clear pass/fail result
- `run_tests` must fail with a non-zero exit code if any compatibility test fails.
- `run_tests` must not require any external services.

==============================
Required workspace outputs
==============================
- run_tests (script or executable)
- tests/test_api_compatibility.py
- docs/analysis.md
- docs/compatibility_strategy.md
- README.md (how to run `run_tests`)

Testing requirements:
- All tests must be runnable via `run_tests`.
- Failures must clearly indicate which compatibility rule was violated.
- Avoid placeholders and pseudo-code; write concrete assertions.

Final step:
After writing all required files, execute `run_tests`
and report whether all tests pass or which tests fail.
